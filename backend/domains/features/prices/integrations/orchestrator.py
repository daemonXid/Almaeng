"""
ğŸ•·ï¸ Crawler Orchestrator

ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ í†µí•©í•˜ì—¬ ê°€ê²© ìˆ˜ì§‘ ë° DB ì €ì¥.
"""

import asyncio
from decimal import Decimal

from django.utils import timezone

from ..models import PriceHistory
from .base import CrawlResult
from .coupang import CoupangCrawler
from .danawa import DanawaCrawler
from .iherb import IHerbCrawler


class CrawlerOrchestrator:
    """í¬ë¡¤ëŸ¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self):
        self.crawlers = {
            "iherb": IHerbCrawler(),
            "danawa": DanawaCrawler(),
            "coupang": CoupangCrawler(),
        }

    async def search_all(self, keyword: str) -> dict[str, list[CrawlResult]]:
        """ëª¨ë“  í”Œë«í¼ì—ì„œ ê²€ìƒ‰"""
        results = {}
        tasks = []

        for name, crawler in self.crawlers.items():
            tasks.append(self._search_with_name(name, crawler, keyword))

        search_results = await asyncio.gather(*tasks, return_exceptions=True)

        for name, result in zip(self.crawlers.keys(), search_results, strict=False):
            if isinstance(result, Exception):
                results[name] = []
            else:
                results[name] = result

        return results

    async def _search_with_name(self, name: str, crawler, keyword: str) -> list[CrawlResult]:
        """ì´ë¦„ê³¼ í•¨ê»˜ ê²€ìƒ‰ ì‹¤í–‰"""
        return await crawler.search(keyword)

    async def get_prices(self, urls: dict[str, str]) -> dict[str, CrawlResult]:
        """í”Œë«í¼ë³„ URLì—ì„œ ê°€ê²© ìˆ˜ì§‘

        Args:
            urls: {"iherb": "https://...", "coupang": "https://..."}
        """
        results = {}
        tasks = []

        for platform, url in urls.items():
            if platform in self.crawlers:
                tasks.append(self._get_price_with_platform(platform, url))

        price_results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, platform in enumerate(urls.keys()):
            result = price_results[i] if i < len(price_results) else None
            if isinstance(result, Exception) or result is None:
                results[platform] = None
            else:
                results[platform] = result

        return results

    async def _get_price_with_platform(self, platform: str, url: str) -> CrawlResult | None:
        """í”Œë«í¼ìœ¼ë¡œ ê°€ê²© ìˆ˜ì§‘"""
        crawler = self.crawlers.get(platform)
        if crawler:
            return await crawler.get_price(url)
        return None

    def save_prices(self, supplement_id: int, results: dict[str, CrawlResult]) -> list[PriceHistory]:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        saved = []

        for platform, result in results.items():
            if result and result.price > 0:
                price_history = PriceHistory.objects.create(
                    supplement_id=supplement_id,
                    platform=platform,
                    price=result.price,
                    original_price=result.original_price or Decimal(0),
                    discount_percent=result.discount_percent or 0,
                    url=result.url,
                    is_in_stock=result.is_in_stock,
                    recorded_at=timezone.now(),
                )
                saved.append(price_history)

        return saved

    async def crawl_and_save(self, supplement_id: int, urls: dict[str, str]) -> list[PriceHistory]:
        """í”Œë«í¼ë³„ URLì—ì„œ ê°€ê²© ìˆ˜ì§‘ í›„ ì €ì¥"""
        results = await self.get_prices(urls)
        return self.save_prices(supplement_id, results)

    async def close_all(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ì¢…ë£Œ"""
        for crawler in self.crawlers.values():
            await crawler.close()


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_orchestrator: CrawlerOrchestrator | None = None


def get_orchestrator() -> CrawlerOrchestrator:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _orchestrator
    if _orchestrator is None:
        _orchestrator = CrawlerOrchestrator()
    return _orchestrator


async def crawl_supplement_prices(supplement_id: int, urls: dict[str, str]) -> list[PriceHistory]:
    """ì˜ì–‘ì œ ê°€ê²© í¬ë¡¤ë§ (ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤)"""
    orchestrator = get_orchestrator()
    return await orchestrator.crawl_and_save(supplement_id, urls)
